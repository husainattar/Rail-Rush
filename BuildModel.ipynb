{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BuildModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+VbxJxLvVIw8yeYHkvXlK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/husainattar/Rail-Rush/blob/master/BuildModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX0DAqR28w6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43UdMTJfIoL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import load_img,img_to_array\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers import *\n",
        "from keras import backend as K\n",
        "from keras.models import model_from_json\n",
        "from matplotlib import cm as CM\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import scipy.io as io\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import PIL\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import random\n",
        "import math\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D14ODldlJjAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "root='/content/drive'\n",
        "add='My Drive/Colab Notebooks/Shanghai-Dataset'\n",
        "partA_train=os.path.join(root,add,'part_A_final/train_data','images')\n",
        "partA_test = os.path.join(root,add,'part_A_final/test_data','images')\n",
        "partB_train = os.path.join(root,add,'part_B_final/train_data','images')\n",
        "partB_test = os.path.join(root,add,'part_B_final/test_data','images')\n",
        "temp = 'test_images'\n",
        "path_sets = [partA_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWv8XchMP6Y9",
        "colab_type": "code",
        "outputId": "527eeac2-755b-4129-997e-e827948258c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#creating array of paths of image data\n",
        "img_paths = []\n",
        "\n",
        "for path in path_sets:\n",
        "    \n",
        "    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n",
        "        \n",
        "        img_paths.append(str(img_path))\n",
        "        \n",
        "print(\"Total images : \",len(img_paths))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total images :  300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD8GaZSuM8X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_img(path):\n",
        "    #Function to load,normalize and return image \n",
        "    im = Image.open(path).convert('RGB')\n",
        "    \n",
        "    im = np.array(im)\n",
        "    \n",
        "    im = im/255.0\n",
        "    \n",
        "    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n",
        "    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n",
        "    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n",
        "\n",
        "    #print(im.shape)\n",
        "    #im = np.expand_dims(im,axis  = 0)\n",
        "    return im\n",
        "\n",
        "def get_input(path):\n",
        "    path = path[0] \n",
        "    img = create_img(path)\n",
        "    return(img)\n",
        "    \n",
        "    \n",
        "    \n",
        "def get_output(path):\n",
        "    #import target\n",
        "    #resize target\n",
        "    \n",
        "    gt_file = h5py.File(path,'r')\n",
        "    \n",
        "    target = np.asarray(gt_file['density'])\n",
        "    \n",
        "    img = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n",
        "    \n",
        "    img = np.expand_dims(img,axis  = 3)\n",
        "    \n",
        "    #print(img.shape)\n",
        "    \n",
        "    return img\n",
        "    \n",
        "    \n",
        "    \n",
        "def preprocess_input(image,target):\n",
        "    #crop image\n",
        "    #crop target\n",
        "    #resize target\n",
        "    crop_size = (int(image.shape[0]/2),int(image.shape[1]/2))\n",
        "    \n",
        "    \n",
        "    if random.randint(0,9)<= -1:            \n",
        "            dx = int(random.randint(0,1)*image.shape[0]*1./2)\n",
        "            dy = int(random.randint(0,1)*image.shape[1]*1./2)\n",
        "    else:\n",
        "            dx = int(random.random()*image.shape[0]*1./2)\n",
        "            dy = int(random.random()*image.shape[1]*1./2)\n",
        "\n",
        "    #print(crop_size , dx , dy)\n",
        "    img = image[dx : crop_size[0]+dx , dy:crop_size[1]+dy]\n",
        "    \n",
        "    target_aug = target[dx:crop_size[0]+dx,dy:crop_size[1]+dy]\n",
        "    #print(img.shape)\n",
        "\n",
        "    return(img,target_aug)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIT_ut0JM_3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Image data generator \n",
        "def image_generator(files, batch_size = 64):\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        input_path = np.random.choice(a = files, size = batch_size)\n",
        "        \n",
        "        batch_input = []\n",
        "        batch_output = [] \n",
        "          \n",
        "        #for input_path in batch_paths:\n",
        "        \n",
        "        inputt = get_input(input_path )\n",
        "        output = get_output(input_path[0].replace('.jpg','.h5').replace('images','ground') )\n",
        "            \n",
        "       \n",
        "        batch_input += [inputt]\n",
        "        batch_output += [output]\n",
        "        \n",
        "\n",
        "        batch_x = np.array( batch_input )\n",
        "        batch_y = np.array( batch_output )\n",
        "        \n",
        "        yield( batch_x, batch_y )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwELovq3vlcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights_vgg(model):\n",
        "    #vgg =  VGG16(weights='imagenet', include_top=False)\n",
        "    \n",
        "    json_file = open('/content/drive/My Drive/Colab Notebooks/VGG_Model/VGG_16.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    loaded_model.load_weights('/content/drive/My Drive/Colab Notebooks/VGG_Model/VGG_16.h5')\n",
        "    \n",
        "    vgg = loaded_model\n",
        "    \n",
        "    vgg_weights=[]                         \n",
        "    for layer in vgg.layers:\n",
        "        if('conv' in layer.name):\n",
        "            vgg_weights.append(layer.get_weights())\n",
        "    \n",
        "    \n",
        "    offset=0\n",
        "    i=0\n",
        "    while(i<10):\n",
        "        if('conv' in model.layers[i+offset].name):\n",
        "            model.layers[i+offset].set_weights(vgg_weights[i])\n",
        "            i=i+1\n",
        "            #print('h')\n",
        "            \n",
        "        else:\n",
        "            offset=offset+1\n",
        "\n",
        "    return (model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIpgrmJ8MlAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    # Euclidean distance as a measure of loss (Loss function) \n",
        "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccYVEIjLMoUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CrowdNet():  \n",
        "            #Variable Input Size\n",
        "            rows = None\n",
        "            cols = None\n",
        "            \n",
        "            #Batch Normalisation option\n",
        "            \n",
        "            batch_norm = 0\n",
        "            kernel = (3, 3)\n",
        "            init = RandomNormal(stddev=0.01)\n",
        "            model = Sequential() \n",
        "            \n",
        "            #custom VGG:\n",
        "            \n",
        "            if(batch_norm):\n",
        "                model.add(Conv2D(64, kernel_size = kernel, input_shape = (rows,cols,3),activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(MaxPooling2D(strides=2))\n",
        "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(MaxPooling2D(strides=2))\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(MaxPooling2D(strides=2))            \n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                \n",
        "            else:\n",
        "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same',input_shape = (rows, cols, 3), kernel_initializer = init))\n",
        "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(MaxPooling2D(strides=2))\n",
        "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(MaxPooling2D(strides=2))\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(MaxPooling2D(strides=2))            \n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                \n",
        "                \n",
        "\n",
        "                \n",
        "            #Conv2D\n",
        "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(256, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(128, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(64, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(1, (1, 1), activation='relu', dilation_rate = 1, kernel_initializer = init, padding = 'same'))\n",
        "        \n",
        "            sgd = SGD(lr = 1e-7, decay = (5*1e-4), momentum = 0.95)\n",
        "            model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])\n",
        "            \n",
        "            model = init_weights_vgg(model)\n",
        "            \n",
        "            return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z43QImwMwUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CrowdNet()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDCMByE3Nclh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEEHj2aYNruG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = image_generator(img_paths,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omu3wXJNNvft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGD(lr = 1e-7, decay = (5*1e-4), momentum = 0.95)\n",
        "model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoJ0oVSYNzk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_mod(model , str1 , str2):\n",
        "    model.save_weights(str1)\n",
        "    \n",
        "    model_json = model.to_json()\n",
        "    \n",
        "    with open(str2, \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "\n",
        "model.fit_generator(train_gen,epochs=200,steps_per_epoch= 700 , verbose=1)\n",
        "\n",
        "save_mod(model,'/content/drive/My Drive/Colab Notebooks/MyModel/model_A_weights.h5','/content/drive/My Drive/Colab Notebooks/MyModel/Model.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIBXCXW3Vy71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_mod(model , str1 , str2):\n",
        "    model.save_weights(str1)\n",
        "    \n",
        "    model_json = model.to_json()\n",
        "    \n",
        "    with open(str2, \"w\") as json_file:\n",
        "        json_file.write(model_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYEDD3boV9yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_mod(model,'/content/drive/My Drive/Colab Notebooks/MyModel/model_A_weights.h5','/content/drive/My Drive/Colab Notebooks/MyModel/Model.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdbofY4OZDhu",
        "colab_type": "code",
        "outputId": "b8e71ad6-1762-4ef1-c78c-cf7904af56c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "file_path=img_paths[22]\n",
        "#im = cv2.imread(file_path)\n",
        "#im=cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
        "# im = Image.open(file_path).convert('RGB')\n",
        "# im = np.array(im)\n",
        "# im = im/255.\n",
        "\n",
        "# im[:,:,0]=(im[:,:,0]-0.485)/0.229\n",
        "# im[:,:,1]=(im[:,:,1]-0.456)/0.224\n",
        "# im[:,:,2]=(im[:,:,2]-0.406)/0.225\n",
        "\n",
        "\n",
        "gt_file = h5py.File(file_path.replace('.jpg','.h5').replace('images','ground'),'r')\n",
        "    \n",
        "target = np.asarray(gt_file['density'])\n",
        "#print(target.shape)\n",
        "img = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n",
        "    \n",
        "img = np.expand_dims(img,axis  = 3)\n",
        "print(img.shape)\n",
        "# input_paths=np.random.choice(a=img_paths,size=64)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 86, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}